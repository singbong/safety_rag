from typing import Optional, List, Any, Dict
from langchain.schema import Document
from tqdm import tqdm
import os
import hashlib
import logging
from datetime import datetime
from dotenv import load_dotenv
import vertexai
from vertexai.generative_models import GenerativeModel
from google import genai
from google.genai.types import Content as GenaiContent, CreateCachedContentConfig, Part as GenaiPart
import warnings
warnings.filterwarnings("ignore")
from google.oauth2 import service_account



os.environ["GOOGLE_GENAI_USE_VERTEXAI"] = "True"
os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = os.getenv("GOOGLE_APPLICATION_CREDENTIALS")
os.environ["GOOGLE_API_KEY"] = os.getenv("GOOGLE_API_KEY")
os.environ["PROJECT_ID"] = os.getenv("PROJECT_ID")
os.environ["GOOGLE_CLOUD_LOCATION"] = "asia-northeast1"



# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Anthropic's Contextual Enrichment Prompt (Korean)
SYSTEM_PROMPT = """
You are an expert at providing brief context to document chunks. Your task is to provide specific contextual information that explains what this chunk is referring to, including concrete details from the full document.
Always respond in Korean language only.
"""


def load_text_from_file(text_file_path: str) -> str:
    """
    original_full_text ë””ë ‰í† ë¦¬ì—ì„œ í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë“œ
    
    Args:
        text_file_path: í…ìŠ¤íŠ¸ íŒŒì¼ ê²½ë¡œ
        
    Returns:
        ì „ì²´ ë¬¸ì„œ í…ìŠ¤íŠ¸
    """
    text_file_path = "../data/original_full_text/" + text_file_path + ".txt"


    if not os.path.exists(text_file_path):
        raise FileNotFoundError(f"í…ìŠ¤íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {text_file_path}")
    
    with open(text_file_path, 'rb') as f:
        full_text = f.read().decode('utf-8')
        
    logger.info(f"í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {text_file_path} ({len(full_text)} ë¬¸ì)")
    return full_text


def count_tokens(text: str, model: str = "gemini-1.5-flash") -> int:
    """
    Google AIì˜ GenerativeModelì„ ì‚¬ìš©í•˜ì—¬ ì •í™•í•œ í† í° ìˆ˜ ê³„ì‚°
    semantic_split_vertex.pyì˜ _count_tokens ë°©ì‹ì„ ë”°ë¦„
    
    Args:
        text: í† í°ì„ ê³„ì‚°í•  í…ìŠ¤íŠ¸
        model: ì‚¬ìš©í•  ëª¨ë¸ëª…
        
    Returns:
        í† í° ìˆ˜
    """
    try:
        # genai ëª¨ë“ˆì´ ì„¤ì •ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        if google_api_key:
            import google.generativeai as genai_count
            genai_count.configure(api_key=google_api_key)
            model_instance = genai_count.GenerativeModel(model)
            count_response = model_instance.count_tokens(text)
            return count_response.total_tokens
        else:
            logger.warning("GOOGLE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ ì‘ì—…ì„ ì¤‘ì§€í•©ë‹ˆë‹¤.")
            raise
    except Exception as e:
        logger.warning(f"í† í° ê³„ì‚° ì˜¤ë¥˜ ì‘ì—… ì¤‘ì§€")
        raise

def check_document_size(text: str, max_tokens: int = 131000) -> tuple[bool, int]:
    """
    ë¬¸ì„œ í¬ê¸°ë¥¼ í† í° ë‹¨ìœ„ë¡œ ì²´í¬í•˜ì—¬ ìºì‹œ ìƒì„± ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
    
    Args:
        text: ì²´í¬í•  í…ìŠ¤íŠ¸
        max_tokens: ìµœëŒ€ í—ˆìš© í† í° ìˆ˜ (Gemini ìºì‹œ ì œí•œ: 131,072)
        
    Returns:
        (í¬ê¸°_ì í•©_ì—¬ë¶€, ì‹¤ì œ_í† í°_ìˆ˜) íŠœí”Œ
    """
    actual_tokens = count_tokens(text)
    is_valid = actual_tokens <= max_tokens
    
    if not is_valid:
        logger.warning(f"âš ï¸ ë¬¸ì„œê°€ í† í° ì œí•œì„ ì´ˆê³¼í•©ë‹ˆë‹¤: {actual_tokens:,} í† í° (ì œí•œ: {max_tokens:,} í† í°)")
        logger.warning(f"   ë¬¸ì„œ ê¸¸ì´: {len(text):,} ê¸€ì")
    else:
        logger.info(f"âœ… ë¬¸ì„œ í¬ê¸° í™•ì¸ ì™„ë£Œ: {actual_tokens:,} í† í° (ì œí•œ: {max_tokens:,} í† í°)")
    
    return is_valid, actual_tokens

def generate_cache_key(text: str, prefix: str = "pdf_context") -> str:
    """
    í…ìŠ¤íŠ¸ì˜ í•´ì‹œê°’ì„ ê¸°ë°˜ìœ¼ë¡œ ìºì‹œ í‚¤ ìƒì„±
    
    Args:
        text: ìºì‹±í•  í…ìŠ¤íŠ¸
        prefix: ìºì‹œ í‚¤ ì ‘ë‘ì‚¬
        
    Returns:
        ìºì‹œ í‚¤
    """
    text_hash = hashlib.sha256(text.encode('utf-8')).hexdigest()[:16]
    timestamp = datetime.now().strftime("%Y%m%d")
    return f"{prefix}_{timestamp}_{text_hash}"

def create_cached_content(full_text: str, cache_ttl_hours: int = 1) -> str:
    """
    Google GenAI APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì „ì²´ ë¬¸ì„œ í…ìŠ¤íŠ¸ ìºì‹±
    
    Args:
        full_text: ìºì‹±í•  ì „ì²´ ë¬¸ì„œ í…ìŠ¤íŠ¸
        cache_ttl_hours: ìºì‹œ TTL (ì‹œê°„)
        
    Returns:
        ìºì‹œ ì´ë¦„(ID)
        
    Raises:
        ValueError: í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ê±°ë‚˜ í† í° ì œí•œì„ ì´ˆê³¼í•˜ëŠ” ê²½ìš°
        Exception: ìºì‹œ ìƒì„± ì‹¤íŒ¨
    """
    # ìµœì†Œ í† í° ìš”êµ¬ì‚¬í•­ ì²´í¬ (ëŒ€ëµ 1024í† í° = 1536ë¬¸ì)
    if len(full_text) < 1536:
        raise ValueError(f"í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤ ({len(full_text)} ë¬¸ì, ìµœì†Œ 1536 ë¬¸ì í•„ìš”)")
    
    # ë¬¸ì„œ í¬ê¸° ì²´í¬ (í† í° ì œí•œ í™•ì¸)
    is_valid, actual_tokens = check_document_size(full_text, max_tokens=131072)
    if not is_valid:
        raise ValueError(f"ë¬¸ì„œê°€ í† í° ì œí•œì„ ì´ˆê³¼í•©ë‹ˆë‹¤: {actual_tokens:,} í† í° (ì œí•œ: 131,072 í† í°). ë¬¸ì„œë¥¼ ë¶„í• í•˜ì—¬ ì²˜ë¦¬í•˜ì„¸ìš”.")
    
    # GenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (Vertex AI ëª¨ë“œ)
    client = genai.Client(
        vertexai=True,
        project=os.getenv("GOOGLE_CLOUD_PROJECT"),
        location=os.getenv("GOOGLE_CLOUD_LOCATION", "asia-northeast1")
    )
    
    # ìºì‹œ í‚¤ ìƒì„±
    cache_key = generate_cache_key(full_text)
    
    # ë¬¸ì„œ ë‚´ìš©ì„ ì»¨í…ì¸ ë¡œ êµ¬ì„±
    contents = [
        GenaiContent(
            role="user",
            parts=[GenaiPart(text=f"ë‹¤ìŒì€ ì „ì²´ ë¬¸ì„œì˜ ë‚´ìš©ì…ë‹ˆë‹¤:\n\n{full_text}")]
        )
    ]
    
    # ìºì‹œ ìƒì„±
    content_cache = client.caches.create(
        model="gemini-2.5-flash-lite",
        config=CreateCachedContentConfig(
            contents=contents,
            system_instruction=SYSTEM_PROMPT,
            display_name=cache_key,
            ttl=f"{int(cache_ttl_hours * 3600)}s",
        )
    )
    
    logger.info(f"ë¬¸ì„œ ìºì‹± ì™„ë£Œ: {cache_key}")
    logger.info(f"ìºì‹œ ì´ë¦„: {content_cache.name}")
    if hasattr(content_cache, 'usage_metadata') and content_cache.usage_metadata:
        logger.info(f"í† í° ìˆ˜: {content_cache.usage_metadata.total_token_count}")
    logger.info(f"TTL: {cache_ttl_hours}ì‹œê°„")
    
    return content_cache.name

def get_or_create_cache(
    document_name: Optional[str] = None,
    text_file_path: Optional[str] = None,
    full_text: Optional[str] = None,
    cache_ttl_hours: int = 1
) -> tuple[Optional[str], str]:
    """
    ê¸°ì¡´ ìºì‹œë¥¼ ì°¾ê±°ë‚˜ ìƒˆë¡œìš´ ìºì‹œ ìƒì„±
    
    Args:
        document_name: ë¬¸ì„œëª… (original_full_textì—ì„œ í•´ë‹¹ í…ìŠ¤íŠ¸ íŒŒì¼ ê²€ìƒ‰)
        text_file_path: ì§ì ‘ ì§€ì •ëœ í…ìŠ¤íŠ¸ íŒŒì¼ ê²½ë¡œ
        full_text: ì§ì ‘ ì œê³µëœ ì „ì²´ í…ìŠ¤íŠ¸
        cache_ttl_hours: ìºì‹œ TTL (ì‹œê°„)
        
    Returns:
        (ìºì‹œ ì´ë¦„ ë˜ëŠ” None, ì „ì²´ í…ìŠ¤íŠ¸) íŠœí”Œ
        ìºì‹œ ì´ë¦„ì´ Noneì´ë©´ í† í° ì œí•œ ì´ˆê³¼ë¡œ ìºì‹œ ìƒì„± ë¶ˆê°€
    """
    # ì „ì²´ í…ìŠ¤íŠ¸ í™•ë³´ ìš°ì„ ìˆœìœ„: full_text > text_file_path > document_name
    if full_text is None:
        if text_file_path is not None:
            full_text = load_text_from_file(text_file_path)
        elif document_name is not None:
            full_text = load_text_from_file(document_name)
        else:
            raise ValueError("document_name, text_file_path ë˜ëŠ” full_text ì¤‘ í•˜ë‚˜ëŠ” ë°˜ë“œì‹œ ì œê³µë˜ì–´ì•¼ í•©ë‹ˆë‹¤")
    
    # ë¬¸ì„œ í¬ê¸° ì‚¬ì „ ì²´í¬
    is_valid, actual_tokens = check_document_size(full_text, max_tokens=131000)
    if not is_valid:
        logger.warning(f"ğŸ“„ ë¬¸ì„œê°€ í† í° ì œí•œì„ ì´ˆê³¼í•˜ì—¬ ìºì‹œë¥¼ ìƒì„±í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {actual_tokens:,} í† í°")
        logger.warning(f"   context_enrichmentë¥¼ Noneìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤")
        return None, full_text
    
    # ê¸°ì¡´ ìºì‹œ ê²€ìƒ‰
    cache_key = generate_cache_key(full_text)
    
    # GenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    client = genai.Client(
        vertexai=True,
        project=os.getenv("GOOGLE_CLOUD_PROJECT"),
        location=os.getenv("GOOGLE_CLOUD_LOCATION", "asia-northeast1")
    )
    
    # ê¸°ì¡´ ìºì‹œ ì¡°íšŒ
    try:
        cached_contents = client.caches.list()
        
        for cached_content in cached_contents:
            if cached_content.display_name == cache_key:
                logger.info(f"ê¸°ì¡´ ìºì‹œ ë°œê²¬: {cache_key}")
                return cached_content.name, full_text
        
        logger.info(f"ê¸°ì¡´ ìºì‹œ ì—†ìŒ. ìƒˆ ìºì‹œ ìƒì„±: {cache_key}")
    except Exception as e:
        logger.warning(f"ìºì‹œ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}. ìƒˆ ìºì‹œ ìƒì„±")
    
    # ìƒˆ ìºì‹œ ìƒì„±
    cache_name = create_cached_content(full_text, cache_ttl_hours)
    return cache_name, full_text

def contextual_content_with_caching_batch(
    docs: List[Document],
    document_name: Optional[str] = None,
    text_file_path: Optional[str] = None,
    full_text: Optional[str] = None,
    model: Optional[GenerativeModel] = "gemini-1.5-flash",
    batch_size: int = 5,
    cache_ttl_hours: int = 1,
    show_progress: bool = True
) -> List[Dict[str, Any]]:
    """
    ìºì‹±ëœ ì „ì²´ ë¬¸ì„œë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©í•œ ë°°ì¹˜ ì²˜ë¦¬ ë§¥ë½ ì •ë³´ ìƒì„±
    
    Args:
        docs: ë¬¸ì„œ ì²­í¬ ë¦¬ìŠ¤íŠ¸
        document_name: ë¬¸ì„œëª… (original_full_textì—ì„œ í•´ë‹¹ í…ìŠ¤íŠ¸ íŒŒì¼ ê²€ìƒ‰)
        text_file_path: ì§ì ‘ ì§€ì •ëœ í…ìŠ¤íŠ¸ íŒŒì¼ ê²½ë¡œ
        full_text: ì§ì ‘ ì œê³µëœ ì „ì²´ í…ìŠ¤íŠ¸
        model: Gemini ëª¨ë¸ (Noneì´ë©´ ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©)
        batch_size: í•œ ë²ˆì— ì²˜ë¦¬í•  ì²­í¬ ìˆ˜
        cache_ttl_hours: ìºì‹œ TTL (ì‹œê°„)
        show_progress: ì§„í–‰ë¥  í‘œì‹œ ì—¬ë¶€
        
    Returns:
        JSON í˜•íƒœì˜ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
    """
    if not docs:
        return []
    
    # ë¬¸ì„œ ì •ë ¬
    logger.info(f"ì •ë ¬ ì „ ë¬¸ì„œ ê°œìˆ˜: {len(docs)}")
    sorted_docs = sorted(docs, key=lambda doc: (
        doc.metadata.get('page', 0), 
        doc.metadata.get('chunk_index', 0)
    ))
    
    # ìºì‹œëœ ì½˜í…ì¸  ìƒì„±/ì¡°íšŒ
    cache_name, _ = get_or_create_cache(
        document_name=document_name,
        text_file_path=text_file_path,
        full_text=full_text, 
        cache_ttl_hours=cache_ttl_hours
    )
    
    # ìºì‹œê°€ Noneì´ë©´ ëª¨ë“  ì²­í¬ì˜ context_enrichmentë¥¼ Noneìœ¼ë¡œ ì„¤ì •
    if cache_name is None:
        logger.info(f"í† í° ì œí•œ ì´ˆê³¼ë¡œ ì¸í•´ ëª¨ë“  ì²­í¬ì˜ context_enrichmentë¥¼ Noneìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤")
        enriched_docs = []
        for i, doc in enumerate(sorted_docs):
            enhanced_doc = {
                "page_content": doc.page_content,
                "metadata": doc.metadata,
                "context_enrichment": None
            }
            enriched_docs.append(enhanced_doc)
        logger.info(f"ì „ì²´ ì²˜ë¦¬ ì™„ë£Œ: {len(enriched_docs)}ê°œ ì²­í¬ (context_enrichment: None)")
        return enriched_docs
    
    # ìºì‹œëœ ì½˜í…ì¸ ë¡œ ëª¨ë¸ ì´ˆê¸°í™”
    if model is None:
        vertexai.init(
            location="asia-northeast1"
        )
        model = GenerativeModel.from_cached_content(cache_name)
        logger.info(f"ìºì‹±ëœ ì½˜í…ì¸ ë¥¼ ì‚¬ìš©í•œ ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ: {cache_name}")
    
    enriched_docs = []
    total_batches = (len(sorted_docs) + batch_size - 1) // batch_size
    
    if show_progress:
        batch_iterator = tqdm(range(total_batches), desc=f"ìºì‹±ëœ ì»¨í…ìŠ¤íŠ¸ ë°°ì¹˜ ì²˜ë¦¬ (ë°°ì¹˜í¬ê¸°={batch_size})")
    else:
        batch_iterator = range(total_batches)
    
    for batch_idx in batch_iterator:
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, len(sorted_docs))
        batch_docs = sorted_docs[start_idx:end_idx]
        
        try:
            # ë°°ì¹˜ í”„ë¡¬í”„íŠ¸ êµ¬ì„± (ìºì‹±ëœ ì „ì²´ ë¬¸ì„œë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©)
            batch_prompts = []
            for i, doc in enumerate(batch_docs):
                global_idx = start_idx + i
                
                prompt_part = f"""
                    ì²­í¬ {global_idx + 1}:
                    <chunk>
                    {doc.page_content}
                    </chunk>
                """
                batch_prompts.append(prompt_part)
            
            # ì „ì²´ ë°°ì¹˜ í”„ë¡¬í”„íŠ¸ (ìºì‹±ëœ ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ í™œìš©)
            full_batch_prompt = f"""
                ìœ„ì˜ ìºì‹±ëœ ì „ì²´ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬, ê° ì²­í¬ì— ëŒ€í•œ ê°„ê²°í•œ í•œ ë¬¸ì¥ì˜ ë§¥ë½ ìš”ì•½ì„ ì œê³µí•´ì£¼ì„¸ìš”. 
                ì „ì²´ ë¬¸ì„œì˜ êµ¬ì²´ì ì¸ ì„¸ë¶€ì‚¬í•­(ì´ë¦„, ë‚ ì§œ, ì¥ì†Œ, ì‚¬ê±´)ì„ í¬í•¨í•˜ì—¬ ê²€ìƒ‰ì— ìœ ìš©í•œ ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”.

                ì˜ˆì‹œ: ì²­í¬ê°€ "í•´ë‹¹ ì‚¬ê³  ì´í›„ 5ì‹œê°„ ë§Œì— ë³µêµ¬ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆë‹¤"ë¼ë©´, 
                ì»¨í…ìŠ¤íŠ¸ëŠ” "2020ë…„ 3ì›” 1ì¼ ê°•ì›ë„ì—ì„œ ë°œìƒí•œ 5ì¤‘ ì¶”ëŒ ì‚¬ê³ ì˜ ë³µêµ¬ ê³¼ì •"ì´ì–´ì•¼ í•©ë‹ˆë‹¤.

                ë‹¤ìŒ {len(batch_docs)}ê°œì˜ ì²­í¬ë¥¼ ì²˜ë¦¬í•´ì£¼ì„¸ìš”:

                {"".join(batch_prompts)}

                ì •í™•íˆ {len(batch_docs)}ê°œì˜ í•œêµ­ì–´ ë¬¸ì¥ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”. ê° ì²­í¬ë§ˆë‹¤ ìˆœì„œëŒ€ë¡œ í•˜ë‚˜ì”©:
                1. [ì²« ë²ˆì§¸ ì²­í¬ ì»¨í…ìŠ¤íŠ¸]
                2. [ë‘ ë²ˆì§¸ ì²­í¬ ì»¨í…ìŠ¤íŠ¸]
                ...

                "ì´ ì²­í¬ëŠ”"ì´ë‚˜ "ì´ ë¶€ë¶„ì€" ê°™ì€ í‘œí˜„ì€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.
            """
            
            # Gemini API í˜¸ì¶œ (ìºì‹±ëœ ì»¨í…ìŠ¤íŠ¸ í™œìš©)
            response = model.generate_content(full_batch_prompt)

            response_text = response.text.strip()
            
            # ì‘ë‹µ íŒŒì‹±
            lines = response_text.split('\n')
            contexts = []
            
            for line in lines:
                line = line.strip()
                if line and (line.startswith(str(len(contexts) + 1) + '.') or line.startswith(f"{len(contexts) + 1}.")):
                    # ë²ˆí˜¸ ì œê±°í•˜ê³  ì»¨í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
                    context = line.split('.', 1)[1].strip() if '.' in line else line
                    contexts.append(context)
                    if len(contexts) >= len(batch_docs):
                        break
            
            # ì‘ë‹µì´ ë¶€ì¡±í•œ ê²½ìš° ê¸°ë³¸ê°’ ì±„ìš°ê¸°
            while len(contexts) < len(batch_docs):
                contexts.append("ì „ì²´ ë¬¸ì„œ ë§¥ë½ì—ì„œì˜ ê´€ë ¨ ë‚´ìš©")
            
            # ê²°ê³¼ êµ¬ì„±
            for i, (doc, context) in enumerate(zip(batch_docs, contexts)):
                enhanced_doc = {
                    "page_content": doc.page_content,
                    "metadata": doc.metadata,
                    "context_enrichment": context
                }
                enriched_docs.append(enhanced_doc)
                    
        except Exception as e:
            logger.error(f"ë°°ì¹˜ {batch_idx + 1} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            # í•´ë‹¹ ë°°ì¹˜ì˜ ëª¨ë“  ì²­í¬ë¥¼ ê¸°ë³¸ê°’ìœ¼ë¡œ ì²˜ë¦¬
            for doc in batch_docs:
                enhanced_doc = {
                    "page_content": doc.page_content,
                    "metadata": doc.metadata,
                    "context_enrichment": None
                }
                enriched_docs.append(enhanced_doc)
    
    logger.info(f"ì „ì²´ ì²˜ë¦¬ ì™„ë£Œ: {len(enriched_docs)}ê°œ ì²­í¬")
    return enriched_docs

def contextual_content_with_caching(
    docs: List[Document],
    document_name: Optional[str] = None,
    text_file_path: Optional[str] = None,
    full_text: Optional[str] = None,
    model: Optional[GenerativeModel] = "gemini-2.5-flash-lite",
    cache_ttl_hours: int = 1,
    show_progress: bool = True
) -> List[Dict[str, Any]]:
    """
    ìºì‹±ëœ ì „ì²´ ë¬¸ì„œë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©í•œ ìˆœì°¨ ì²˜ë¦¬ ë§¥ë½ ì •ë³´ ìƒì„±
    
    Args:
        docs: ë¬¸ì„œ ì²­í¬ ë¦¬ìŠ¤íŠ¸
        document_name: ë¬¸ì„œëª… (original_full_textì—ì„œ í•´ë‹¹ í…ìŠ¤íŠ¸ íŒŒì¼ ê²€ìƒ‰)
        text_file_path: ì§ì ‘ ì§€ì •ëœ í…ìŠ¤íŠ¸ íŒŒì¼ ê²½ë¡œ
        full_text: ì§ì ‘ ì œê³µëœ ì „ì²´ í…ìŠ¤íŠ¸
        model: Gemini ëª¨ë¸ (ê¸°ë³¸ê°’: gemini-2.5-flash-lite)
        cache_ttl_hours: ìºì‹œ TTL (ì‹œê°„)
        show_progress: ì§„í–‰ë¥  í‘œì‹œ ì—¬ë¶€
        
    Returns:
        JSON í˜•íƒœì˜ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
    """
    if not docs:
        return []
    
    # ë¬¸ì„œ ì •ë ¬
    logger.info(f"ì •ë ¬ ì „ ë¬¸ì„œ ê°œìˆ˜: {len(docs)}")
    sorted_docs = sorted(docs, key=lambda doc: (
        doc.metadata.get('page', 0), 
        doc.metadata.get('chunk_index', 0)
    ))
    
    # ìºì‹œëœ ì½˜í…ì¸  ìƒì„±/ì¡°íšŒ
    cache_name, _ = get_or_create_cache(
        document_name=document_name,
        text_file_path=text_file_path,
        full_text=full_text, 
        cache_ttl_hours=cache_ttl_hours
    )
    
    # ìºì‹œê°€ Noneì´ë©´ ëª¨ë“  ì²­í¬ì˜ context_enrichmentë¥¼ Noneìœ¼ë¡œ ì„¤ì •
    if cache_name is None:
        logger.info(f"í† í° ì œí•œ ì´ˆê³¼ë¡œ ì¸í•´ ëª¨ë“  ì²­í¬ì˜ context_enrichmentë¥¼ Noneìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤")
        enriched_docs = []
        for i, doc in enumerate(sorted_docs):
            enhanced_doc = {
                "page_content": doc.page_content,
                "metadata": doc.metadata,
                "context_enrichment": None
            }
            enriched_docs.append(enhanced_doc)
        logger.info(f"ì „ì²´ ì²˜ë¦¬ ì™„ë£Œ: {len(enriched_docs)}ê°œ ì²­í¬ (context_enrichment: None)")
        return enriched_docs
    
    # ìºì‹œëœ ì½˜í…ì¸ ë¡œ ëª¨ë¸ ì´ˆê¸°í™”
    if model is None:
        vertexai.init(
            project=project_id, 
            location="asia-northeast1",
            credentials=credentials
        )
        model = GenerativeModel.from_cached_content(cache_name)
        logger.info(f"ìºì‹±ëœ ì½˜í…ì¸ ë¥¼ ì‚¬ìš©í•œ ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ: {cache_name}")
    
    iterator = tqdm(enumerate(sorted_docs), desc="ìºì‹±ëœ ì»¨í…ìŠ¤íŠ¸ ìˆœì°¨ ì²˜ë¦¬") if show_progress else enumerate(sorted_docs)
    
    enriched_docs = []
    
    for i, doc in iterator:
        try:
            prompt = f"""
            ìœ„ì˜ ìºì‹±ëœ ì „ì²´ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬, ë‹¤ìŒ ì²­í¬ì— ëŒ€í•œ ê°„ê²°í•œ í•œ ë¬¸ì¥ì˜ ë§¥ë½ ìš”ì•½ì„ ì œê³µí•´ì£¼ì„¸ìš”.
            ì „ì²´ ë¬¸ì„œì˜ êµ¬ì²´ì ì¸ ì„¸ë¶€ì‚¬í•­(ì´ë¦„, ë‚ ì§œ, ì¥ì†Œ, ì‚¬ê±´)ì„ í¬í•¨í•˜ì—¬ ê²€ìƒ‰ì— ìœ ìš©í•œ ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”.

            <chunk>
            {doc.page_content}
            </chunk>

            ì˜ˆì‹œ: ì²­í¬ê°€ "í•´ë‹¹ ì‚¬ê³  ì´í›„ 5ì‹œê°„ ë§Œì— ë³µêµ¬ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆë‹¤"ë¼ë©´, 
            ì»¨í…ìŠ¤íŠ¸ëŠ” "2020ë…„ 3ì›” 1ì¼ ê°•ì›ë„ì—ì„œ ë°œìƒí•œ 5ì¤‘ ì¶”ëŒ ì‚¬ê³ ì˜ ë³µêµ¬ ê³¼ì •"ì´ì–´ì•¼ í•©ë‹ˆë‹¤.

            ì˜¤ì§ í•œ ë¬¸ì¥ì˜ ê°„ê²°í•œ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”. "ì´ ì²­í¬ëŠ”"ì´ë‚˜ "ì´ ë¶€ë¶„ì€" ê°™ì€ í‘œí˜„ì€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.
            """
            
            response = model.generate_content(prompt)
            contextual_info = response.text.strip()
            
            enhanced_doc = {
                "page_content": doc.page_content,
                "metadata": doc.metadata,
                "context_enrichment": contextual_info
            }
            enriched_docs.append(enhanced_doc)
                    
        except Exception as e:
            logger.error(f"ì²­í¬ {i} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise
    
    logger.info(f"ì „ì²´ ì²˜ë¦¬ ì™„ë£Œ: {len(enriched_docs)}ê°œ ì²­í¬")
    return enriched_docs

def cleanup_expired_caches(dry_run: bool = True) -> List[str]:
    """
    ë§Œë£Œëœ ìºì‹œë“¤ì„ ì •ë¦¬
    
    Args:
        dry_run: Trueë©´ ì‚­ì œí•  ìºì‹œë§Œ ë‚˜ì—´, Falseë©´ ì‹¤ì œ ì‚­ì œ
        
    Returns:
        ì²˜ë¦¬ëœ ìºì‹œ ëª©ë¡
    """
    # GenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    client = genai.Client(
        vertexai=True,
        project=project_id,
        location="asia-northeast1",
        credentials=credentials
    )
    
    cached_contents = client.caches.list()
    expired_caches = []
    
    for cached_content in cached_contents:
        try:
            # ìºì‹œ ë§Œë£Œ ì‹œê°„ í™•ì¸
            if hasattr(cached_content, 'expire_time') and cached_content.expire_time:
                current_time = datetime.now()
                expire_time = cached_content.expire_time
                
                # timezone aware datetimeì„ naiveë¡œ ë³€í™˜
                if hasattr(expire_time, 'replace') and expire_time.tzinfo is not None:
                    expire_time = expire_time.replace(tzinfo=None)
                
                if current_time > expire_time:
                    expired_caches.append(cached_content.display_name)
                    if not dry_run:
                        client.caches.delete(cached_content.name)
                        logger.info(f"ë§Œë£Œëœ ìºì‹œ ì‚­ì œ: {cached_content.display_name}")
        except Exception as e:
            logger.warning(f"ìºì‹œ {cached_content.display_name} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    
    if dry_run:
        logger.info(f"ì‚­ì œ ì˜ˆì • ìºì‹œ {len(expired_caches)}ê°œ: {expired_caches}")
    else:
        logger.info(f"ì‚­ì œ ì™„ë£Œ ìºì‹œ {len(expired_caches)}ê°œ")
        
    return expired_caches

if __name__ == "__main__":
    pass